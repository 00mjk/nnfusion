[{"tvm_func_name": "manual_dot_nn_op_float_m1_k256_n256_kernel0", "op_type": "Dot", "parameters": {"arg0_shape": [1, 256], "arg1_shape": [256, 256], "out_shape": [1, 256], "transpose_A": false, "transpose_B": false}, "code": "extern \"C\" __global__ void manual_dot_nn_op_float_m1_k256_n256_kernel0(float* input0, float* input1, float* output0)\n{\n    int warp_id = threadIdx.x >> 5;\n    int lane_id = threadIdx.x & 31;\n    int col_id = blockIdx.x * blockDim.x / 4 + lane_id;\n    if (col_id < 256)\n    {\n        float val = 0;\n        int k_start = warp_id * 64;\n        int k_end = (warp_id + 1) * 64;\n        for (int i = k_start; i < k_end; i++)\n        {\n            val = fma(input0[i], input1[i * 256 + col_id], val);\n        }\n        if (warp_id == 0)\n        {\n            output0[col_id]=0;\n        }\n        __syncthreads();\n        atomicAdd(output0 + col_id, val);\n    }\n\n}\n", "gridDim": [8, 1, 1], "blockDim": [128, 1, 1]}, {"tvm_func_name": "manual_dot_nn_op_float_m1_k256_n512_kernel0", "op_type": "Dot", "parameters": {"arg0_shape": [1, 256], "arg1_shape": [256, 512], "out_shape": [1, 512], "transpose_A": false, "transpose_B": false}, "code": "extern \"C\" __global__ void manual_dot_nn_op_float_m1_k256_n512_kernel0(float* input0, float* input1, float* output0)\n{\n    int warp_id = threadIdx.x >> 5;\n    int lane_id = threadIdx.x & 31;\n    int col_id = blockIdx.x * blockDim.x / 4 + lane_id;\n    if (col_id < 512)\n    {\n        float val = 0;\n        int k_start = warp_id * 64;\n        int k_end = (warp_id + 1) * 64;\n        for (int i = k_start; i < k_end; i++)\n        {\n            val = fma(input0[i], input1[i * 512 + col_id], val);\n        }\n        if (warp_id == 0)\n        {\n            output0[col_id]=0;\n        }\n        __syncthreads();\n        atomicAdd(output0 + col_id, val);\n    }\n\n}", "gridDim": [16, 1, 1], "blockDim": [128, 1, 1]}, {"tvm_func_name": "manual_dot_nn_op_float_m1_k512_n1024_kernel0", "op_type": "Dot", "parameters": {"arg0_shape": [1, 512], "arg1_shape": [512, 1024], "out_shape": [1, 1024], "transpose_A": false, "transpose_B": false}, "code": "extern \"C\" __global__ void manual_dot_nn_op_float_m1_k512_n1024_kernel0(float* input0, float* input1, float* output0)\n{\n    int warp_id = threadIdx.x >> 5;\n    int lane_id = threadIdx.x & 31;\n    int col_id = blockIdx.x * blockDim.x / 8 + lane_id;\n    if (col_id < 1024)\n    {\n        float val = 0;\n        int k_start = warp_id * 64;\n        int k_end = (warp_id + 1) * 64;\n        for (int i = k_start; i < k_end; i++)\n        {\n            val = fma(input0[i], input1[i * 1024 + col_id], val);\n        }\n        if (warp_id == 0)\n        {\n            output0[col_id]=0;\n        }\n        __syncthreads();\n        atomicAdd(output0 + col_id, val);\n    }\n\n}\n", "gridDim": [32, 1, 1], "blockDim": [256, 1, 1]}, {"tvm_func_name": "manual_dot_nn_op_float_m1_k3008_n1024_kernel0", "op_type": "Dot", "parameters": {"arg0_shape": [1, 3008], "arg1_shape": [3008, 1024], "out_shape": [1, 1024], "transpose_A": false, "transpose_B": false}, "code": "extern \"C\" __global__ void manual_dot_nn_op_float_m1_k3008_n1024_kernel0(float* input0, float* input1, float* output0)\n{\n    int warp_id = threadIdx.x >> 5;\n    int lane_id = threadIdx.x & 31;\n    int col_id = blockIdx.x * blockDim.x / 32 + lane_id;\n    if (col_id < 1024)\n    {\n        float val = 0;\n        int k_start = warp_id * 94;\n        int k_end = (warp_id + 1) * 94;\n        for (int i = k_start; i < k_end; i++)\n        {\n            val = fma(input0[i], input1[i * 1024 + col_id], val);\n        }\n        if (warp_id == 0)\n        {\n            output0[col_id]=0;\n        }\n        __syncthreads();\n        atomicAdd(output0 + col_id, val);\n    }\n\n}\n", "gridDim": [32, 1, 1], "blockDim": [1024, 1, 1]}]